<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Eye to Eye Project</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Urbanist:wght@400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../css/styles.css">
</head>

<body>
    <!-- Navigation Bar -->
    <nav class="navbar">
        <div class="container">
            <ul class="nav-menu">
                <li><a href="../index.html">Home</a></li>
                <li><a href="../projects.html">Projects</a></li>
                <li><a href="../resume.html">Resume</a></li>
                <li><a href="../contact.html">Get in Touch</a></li>
            </ul>
        </div>
    </nav>

    <!-- Project Content -->
    <section class="project-detail">
        <div class="container">
            <h1 class="section-title">Eye to Eye Project</h1>

            <style>
                /* Banner container: 80% width, 35% height of viewport */
                .banner-collage {
                    width: 80vw;
                    height: 35vh;
                    display: grid;
                    grid-template-columns: 2fr 1fr;
                    grid-template-rows: 1fr 1fr;
                    gap: 8px;
                    background: #111;
                    padding: 8px;
                    box-sizing: border-box;
                    overflow: hidden;
                }

                /* Main large image (left side, spanning two rows) */
                .banner-main {
                    grid-row: 1 / 3;
                    grid-column: 1 / 2;
                    position: relative;
                    overflow: hidden;
                }

                /* Smaller images (right side, stacked) */
                .banner-side {
                    position: relative;
                    overflow: hidden;
                }

                .banner-collage img {
                    width: 100%;
                    height: 100%;
                    object-fit: cover;
                    display: block;
                }

                /* Optional overlay text styling */
                .overlay-text {
                    position: absolute;
                    bottom: 10px;
                    left: 12px;
                    color: #fff;
                    font-size: 1.1rem;
                    font-weight: 600;
                    text-shadow: 0 2px 4px rgba(0, 0, 0, 0.6);
                }
            </style>
            <div class="banner-collage">
                <!-- Main large image -->
                <div class="banner-main">
                    <img src="../images/Projects/Pandemic/dramatic_eyes.png" alt="Main banner image" />
                    <div class="overlay-text">IEEE Pandemic Challenge</div>
                </div>

                <!-- Top-right small image -->
                <div class="banner-side">
                    <img src="../images/Projects/Pandemic/nn_image.png" alt="Side image 1" />
                </div>

                <!-- Bottom-right small image -->
                <div class="banner-side">
                    <img src="../images/Projects/Pandemic/eye_tracking.png" alt="Side image 2" />
                </div>
            </div>

            <div class="project-content">
                <h2>Eye to Eye Project Background</h2>
                <p>
                    I began developing the Eye to Eye project as a sophomore in high school in 2021. The initial concept
                    proposed solving the problem of maintaining eye contact during video calls through a hardware-based
                    approach using one-way mirrors to redirect a user’s gaze toward their camera. While this design had
                    the potential to be effective, it relied on specialized equipment that would not be readily
                    available to most users. I therefore chose to pursue a software-based solution that could be easily
                    adopted by everyday video conferencing users.
                </p>
                <p>
                    At the time, my programming experience was primarily in Java, JavaScript, HTML, and CSS. My initial
                    approach was to leverage these technologies to build a system that would detect a user’s eyes
                    through their webcam, isolate the eye regions, and overlay replicated eye images onto the user’s
                    full-face video feed. The goal was to apply image warping techniques similar to fisheye distortion
                    to create the illusion that the user was maintaining direct eye contact with the camera.
                </p>

                <h2>Technologies Used</h2>
                <p>
                    Throughout this project, I got to learn about a lot of different computer languages, object
                    detection techniques, machine learning libraries, and algorithms used for object detection. Below
                    are some of the kinds of technology that were used.
                </p>
                <ul>
                    <li>Python</li>
                    <li>Java</li>
                    <li>JavaScript</li>
                    <li>OpenCV Library</li>
                    <li>Haar Cascades</li>
                    <li>Numpy arrays</li>
                    <li>SQL Databases</li>
                </ul>

                <h2>Early Research and Prototype Development</h2>
                <p>
                    While researching methods for real-time face tracking, I discovered an article on <a
                        href="https://medium.com/analytics-vidhya/creating-snapchat-like-filters-from-scratch-using-computer-vision-techniques-6374cde6a7db">The
                        Medium</a> that demonstrated how to create Snapchat-style filters using computer vision
                    techniques. This resource introduced me to the fundamentals of capturing and processing live video
                    feeds. Following this tutorial required extensive self-study, as many of the underlying computer
                    vision concepts were new to me.
                </p>
                <p>
                    During this phase, I relied heavily on adapting and extending examples from tutorials, combined with
                    prior experience manipulating visual elements in web development. I experimented with reshaping
                    video feed regions, adjusting color channels, and dynamically positioning overlay elements. These
                    experiments allowed me to develop a prototype capable of capturing eye-region video feeds and
                    overlaying them onto the user’s face.
                </p>
                <p>However, the overlay rendering introduced performance challenges. Real-time processing created
                    noticeable latency, causing the eye overlays to drift or appear detached from the user’s face during
                    rapid head movement. Recognizing that this level of instability would limit real-world usability, I
                    began researching more robust eye-tracking methods. </p>
                <figure class="intext-img">
                    <img src="../images/Projects/Pandemic/funny_eyes.png" alt="Descriptive alt text">
                    <figcaption>Original eye tracking results; Left: The first attempt at fisheye warp, Right: The eye
                        overlays (highlighted yellow)</figcaption>
                </figure>

                <h2>Transition to Advanced Computer Vision Techniques</h2>
                <p>
                    While exploring more reliable tracking algorithms, I discovered that many advanced computer vision
                    and gaze-tracking implementations were developed using Python. Although I had no prior experience
                    with Python at the time, I chose to expand my skill set and transitioned the project into a
                    Python-based implementation.
                </p>
                <p>
                    During this transition, I received mentorship from the host of the Richmond division of the Pandemic
                    Challenge, who helped guide my learning of Python and introduced me to additional computer vision
                    techniques and libraries.
                </p>
                <p>
                    One of the earliest Python-based iterations of the project utilized <a
                        href="https://medium.com/@utkrisht14/face-detection-using-haar-cascade-algorithm-267751b43240">Haar
                        Cascade classifiers</a> implemented through the OpenCV library. Haar Cascades use supervised
                    machine learning to detect visual features by applying cascaded classifiers trained on positive and
                    negative image samples. In this implementation, the algorithm scanned each frame of the webcam feed
                    using sliding detection windows to identify facial regions and eye locations. Once detected, the
                    system extracted regions of interest (ROIs) corresponding to the eyes, which could then be processed
                    for tracking and visualization.
                </p>

                <h2>Haar Cascade Eye Detection Implementation</h2>
                <p>
                    Haar Cascades operate by identifying patterns of contrast within an image using a collection of
                    simple rectangular feature detectors. These features measure differences in pixel intensity between
                    adjacent regions, allowing the algorithm to recognize patterns which are commonly found in human
                    faces and eyes.
                </p>
                <p>
                    Each Haar feature consists of adjacent rectangular regions. The algorithm subtracts the summed pixel
                    intensities of one region from another. For example, eye regions typically contain darker areas
                    (pupil and eyelashes) surrounded by lighter skin tones. Haar features use this predictable contrast
                    pattern to identify facial features from the surrounding background.
                </p>
                <figure class="intext-img">
                    <img src="../images/Projects/Pandemic/faced.jpeg" alt="Descriptive alt text">
                    <figcaption>Facial features recognized and differentiated from background clutter</figcaption>
                </figure>
                <h2>Cascaded Classification Structure</h2>
                <p>Instead of applying all detection features at once, Haar Cascades use a multi-stage filtering process
                    known as a <b>cascade of classifiers</b>. Each stage acts as a quick rejection filter that
                    determines
                    whether a given region of the image could contain the target object.</p>
                <p>The cascade operates as follows:</p>
                <ul>
                    <li>The algorithm scans the image using a sliding detection window.</li>
                    <li>The first stage applies a small number of simple features to eliminate obvious non-face regions.
                    </li>
                    <li>Regions that pass the first stage are passed to more involved classification stages.</li>
                    <li>If a region successfully passes all stages, it is classified as containing the target feature
                        (such as an eye or face).</li>
                </ul>
                <p>This staged approach allows the algorithm to discard most irrelevant regions early, dramatically
                    improving performance while maintaining detection accuracy.</p>
                <h2>Sliding Window Detection</h2>
                <p>To locate facial features within each webcam frame, the Haar Cascade classifier applies a sliding
                    window across the image at multiple scales. This process allows the algorithm to detect objects of
                    varying sizes depending on the user’s distance from the camera.</p>
                <p>At each window position, the classifier evaluates whether the region contains a face or eye. If the
                    classifier produces a positive detection, the algorithm records the coordinates of that region as a
                    bounding box.</p>

                <figure class="intext-img">
                    <img src="../images/Projects/Pandemic/happy_rectangles.jpeg" alt="Descriptive alt text">
                    <figcaption>Rectangle regions used to indicate patterns that the algorithm uses to identify 
                        possible facial feature in the image such as darker patterns where the eyes are located, 
                        and lighter areas along the nose</figcaption>
                </figure>
                <h2>Region of Interest Extraction</h2>
                <p>Once facial and eye regions were detected, the system extracted these areas as <b>Regions of Interest
                        (ROIs)</b>. These ROIs isolated only the relevant portions of the video feed containing the
                    user’s eyes. The ROIs were then to be processed separately for overlay placement and image
                    manipulation.</p>
                <p>This would enable the project to isolate eye movement data from the rest of the face, which was
                    necessary for later gaze simulation techniques.</p>
                <h2>Training Image Development for Detection</h2>
                <p>To improve detection performance and tailor the model to the specific use cases, we imported a
                    dataset of training images containing thousands of face images to determine what parameters returned
                    the most accurate results. The code used for training and testing the detection pipeline is publicly
                    available <a href="https://github.com/Ferret-Dragon/TrainingImages">here.</a></p>
                <p>The training dataset consists of two primary categories:</p>
                <ul>
                    <li><b>Positive Images:</b> Images containing clearly visible faces that the detector should
                        recognize. These images include variations in lighting, orientation, facial expression, and
                        partial occlusion to increase generalization capability.</li>
                    <li><b>Negative Images:</b> Images that do not contain faces. These are used to train the classifier
                        to distinguish facial features from background textures and non-face objects.</li>
                </ul>
                <h2>Limitations Encountered</h2>
                <p>Despite their speed and simplicity, the Haar Cascades presented several limitations that affected
                    tracking reliability and complexity even after tuning the parameters:</p>

                <b>Sensitivity to Lighting Conditions</b>
                <p>Variations in brightness or shadowing often caused inconsistent detection results, particularly
                    around the eye region.</p>
                <b>Limited Rotation Tolerance</b>
                <p>The classifier performed best when the user faced the camera directly. Head tilting or rotation
                    frequently caused detection failure.</p>

                <h2>Impact on Project Development</h2>
                <p>Although Haar Cascades allowed the Eye to Eye project to achieve its first working eye detection
                    prototype, the limitations in positional stability and environmental robustness made it clear that
                    more advanced tracking methods would be necessary for a production-ready solution.</p>
                <p>This project has been one of the most influential experiences in my technical
                    development. From this, I have learned so much about different programming
                    languages, explored machine learning and computer vision techniques through both independent and collaborative Research. 
                    Seeing how machine learning models and real-time image
                    processing can be combined to solve human-centered communication challenges significantly shaped my
                    academic and professional interests. This experience helped to inspire my passion for computer
                    visualization and reinforced my desire to pursue future opportunities focused on developing and
                    applying computer vision technologies to create accessible and innovative experiences.</p>


                <div class="project-links">
                    <a href="https://github.com/Ferret-Dragon/Eye_to_Eye" class="btn-primary">View on GitHub</a>
                </div>

                <a href="../projects.html" class="read-more">← Back to Projects</a>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <h3 class="footer-title">Follow Along</h3>
            <div class="social-links">
                <a href="https://www.linkedin.com/in/katrina-ashman-1570b3228/" target="_blank"
                    class="social-link">LinkedIn</a>
                <a href="https://github.com/Ferret-Dragon" target="_blank" class="social-link">GitHub</a>
            </div>
        </div>
    </footer>

    <script src="../js/main.js"></script>
</body>

</html>